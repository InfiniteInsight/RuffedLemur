# Database Migration Workflow

This document outlines the workflow for managing database migrations in the RuffedLemur project.

## Migration Philosophy

Migrations in RuffedLemur follow these principles:

1. **Schema First**: Database schema changes should be committed before the code that uses them. This means each migration is a two-step process.
2. **Forward Only**: In production, we only move forward with migrations, never backward.
3. **Idempotent**: Migrations should be designed to be idempotent when possible.
4. **Data Preservation**: Migrations should never result in data loss without explicit approval.
5. **Testing**: All migrations must be tested in a non-production environment before being applied to production.

## Development Workflow

### Creating a New Migration

When you need to make changes to the database schema:

1. **Update Models**: First, update your SQLAlchemy models to reflect the desired schema.

2. **Generate Migration Script**:

   ```bash
   # Using Flask-Migrate CLI
   flask db migrate -m "Description of schema changes"
   
   # Using our CLI
   python -m ruffedlemur.cliApp db make-migration "Description of schema changes"
   ```

3. **Review the Migration**: Carefully review the generated migration script to ensure it does what you expect.

4. **Edit if Necessary**: Sometimes the autogenerated migrations need manual adjustments, especially for:
   - Complex data transformations
   - Default values for new non-nullable columns
   - Specialized index changes
   - Ensure proper sequence of operations

5. **Test the Migration**:

   ```bash
   # Apply the migration
   flask db upgrade
   
   # Verify it works as expected
   
   # Optionally, test rollback (for local dev only)
   flask db downgrade
   ```

6. **Commit Changes**: Commit both the model changes and the migration script.

### Running Migrations

To apply pending migrations:

```bash
# Apply all pending migrations
flask db upgrade

# Or using our CLI
python -m ruffedlemur.cliApp db upgrade
```

### Deployment Strategy

For deployment, we follow this process:

1. **Pre-Deployment**: Backup the database before applying migrations.

2. **Migration Phase**: Apply database migrations before deploying new code.

3. **Application Deployment**: Deploy the new application code.

This ensures that when the new code runs, the database schema is already updated.

## Common Migration Patterns

### Adding a New Table

```python
def upgrade():
    op.create_table(
        'new_table',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, default=uuid.uuid4),
        sa.Column('name', sa.String(128), nullable=False),
        # Add other columns...
    )

def downgrade():
    op.drop_table('new_table')
```

### Adding a Column

```python
def upgrade():
    op.add_column('table_name', sa.Column('new_column', sa.String(128)))

def downgrade():
    op.drop_column('table_name', 'new_column')
```

### Adding a Non-Nullable Column to an Existing Table

```python
def upgrade():
    # First add the column as nullable
    op.add_column('table_name', sa.Column('new_column', sa.String(128), nullable=True))
    
    # Then update existing rows
    op.execute("UPDATE table_name SET new_column = 'default_value'")
    
    # Finally, set the column to non-nullable
    op.alter_column('table_name', 'new_column', nullable=False)

def downgrade():
    op.drop_column('table_name', 'new_column')
```

### Changing Column Type

```python
def upgrade():
    # For PostgreSQL, we can use ALTER COLUMN TYPE
    op.execute('ALTER TABLE table_name ALTER COLUMN column_name TYPE new_type USING column_name::new_type')

def downgrade():
    op.execute('ALTER TABLE table_name ALTER COLUMN column_name TYPE original_type USING column_name::original_type')
```

### Renaming a Column

```python
def upgrade():
    op.alter_column('table_name', 'old_name', new_column_name='new_name')

def downgrade():
    op.alter_column('table_name', 'new_name', new_column_name='old_name')
```

## Troubleshooting

### Migration Conflicts

If you encounter migration conflicts (two developers creating migrations from the same base):

1. Keep the migration that was merged first
2. For the second migration:
   - Update the `down_revision` to point to the first migration
   - Remove any operations that are redundant

### Failed Migrations

If a migration fails:

1. Fix the issue in a new migration rather than editing the failed one
2. Use transaction handling to ensure atomicity where possible

### Performance Considerations

For large tables:

1. Consider batching data migrations
2. Run long-running migrations during maintenance windows
3. Use concurrent indexes in PostgreSQL where possible
4. Consider temporarily disabling triggers during large updates

## Best Practices

1. **One Change Per Migration**: Each migration should do only one logical change.
2. **Descriptive Names**: Use clear, descriptive names for migration files.
3. **Include Both Directions**: Always implement both `upgrade()` and `downgrade()`.
4. **Review Generated SQL**: Use `flask db upgrade --sql` to review the SQL that will be executed.
5. **Version Control**: Always commit migration scripts to version control.
6. **Test Thoroughly**: Test migrations on a copy of production data before applying them to production.

## Examples

- Creating migrations: `python -m ruffedlemur.cliApp db make-migration`
- Upgrading the database: `python -m ruffedlemur.cliApp db upgrade`
- Downgrading the database: `python -m ruffedlemur.cliApp db downgrade`
- Checking migration status: `python -m ruffedlemur.cliApp db current and db history`
- Stamping the database version: `python -m ruffedlemur.cliApp db stamp`
- Merging migrations: `python -m ruffedlemur.cliApp db merge`
- Checking for pending migrations: `python -m ruffedlemur.cliApp db check`
